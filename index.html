<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="IntelliAsk: Learning to Ask Critical Questions with Human-Aligned Rewards. A system for generating high-quality peer review questions using reward models trained on human preferences.">
  <meta name="keywords" content="IntelliAsk, Peer Review, Question Generation, Reward Models, LLM, ACL 2026">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>IntelliAsk: Learning to Ask Critical Questions with Human-Aligned Rewards</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">IntelliAsk: Learning to Ask Critical Questions with Human-Aligned Rewards</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              First Author<sup>1</sup>,</span>
            <span class="author-block">
              Second Author<sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Affiliation / Address line 1,</span>
            <span class="author-block"><sup>2</sup>Affiliation / Address line 1</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Links will be added here when available -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/intelliask/architecture.png" 
           alt="IntelliAsk Architecture" 
           style="width: 100%; max-width: 1000px; margin: 0 auto; display: block; border-radius: 10px;">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">IntelliAsk</span> generates critical, well-reasoned questions for peer review
        using human-aligned reward models.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-architecture">
          <img src="./static/images/intelliask/architecture.png" alt="IntelliAsk Architecture" style="width: 100%; height: auto;">
        </div>
        <div class="item item-reward">
          <img src="./static/images/intelliask/reward_model.png" alt="IntelliReward Architecture" style="width: 100%; height: auto;">
        </div>
        <div class="item item-data">
          <img src="./static/images/intelliask/data_pipeline.png" alt="Data Pipeline" style="width: 100%; height: auto;">
        </div>
        <div class="item item-scores">
          <img src="./static/images/intelliask/score_graph.png" alt="Score Distribution" style="width: 100%; height: auto;">
        </div>
        <div class="item item-curation">
          <img src="./static/images/intelliask/data_curation.png" alt="Data Curation" style="width: 100%; height: auto;">
        </div>
        <div class="item item-waterfall">
          <img src="./static/images/intelliask/waterfall.png" alt="Filtering Pipeline" style="width: 100%; height: auto;">
        </div>
        <div class="item item-training">
          <img src="./static/images/intelliask/sft_vs_rl.png" alt="Training Curves" style="width: 100%; height: auto;">
        </div>
        <div class="item item-preference">
          <img src="./static/images/intelliask/IntelliAskvsOthersPreferences.png" alt="Preference Comparison" style="width: 100%; height: auto;">
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Peer review relies on substantive, evidence-based questions, but existing LLM-based approaches often generate surface-level queries. We find that LLM-generated questions take over 50% of their question tokens from a paper's first page, while human reviewers draw on the full text. Human questions are also more insightful, showing effort and grounding, whereas LLM questions mostly reflect surface style.
          </p>
          <p>
            To address this, we extract 151k candidate questions from ICLR 2024 reviews and filter them through a multi-stage filtering process into Probe-15K, a set of 15.5k high-quality questions. From this, we create ProbeVote-500, where human annotators score questions along effort, evidence, and grounding. Using these labels, we train IntelliReward, a reward model built from a frozen Autoregressive LLM with trainable multi-head transformers over the final 50 token states. This architecture outperforms API-based SFT finetuning (Gemini 2.5 Flash, GPT-4.1) as baselines for reward.
          </p>
          <p>
            Applying DAPO with IntelliReward, we train <span class="dnerf">IntelliAsk</span>, a question-generation model aligned with human preferences and substantially stronger than existing fine-tuned review models. Finally, by releasing Probe-15K, ProbeVote-500, and IntelliReward, we provide an automatic evaluation benchmark for reviewer questions that measures groundedness, effort, and evidence.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Probe-15K Dataset. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Probe-15K Dataset</h2>
        <div class="content has-text-justified">
          <p>
            We collected a dataset of reviewer feedback by scraping all publicly available reviews from ICLR 2024 using the OpenReview API. For each paper, we retrieved the corresponding metadata and downloaded the main PDF (excluding supplementary materials), limiting the maximum length to nine pages.
          </p>
          <p>
            To address variability in question placement, we used Gemini 2.0 to extract questions from the <i>Questions</i>, <i>Strengths</i>, and <i>Weaknesses</i> sections, preserving their original phrasing and tone. The initial extraction produced about 151,000 questions.
          </p>
          <p>
            We applied a series of filtering steps to ensure quality:
          </p>
          <ul>
            <li><strong>Length-Based Filtering:</strong> Excluded questions under 100 characters (removed 34,000 entries)</li>
            <li><strong>Semantic Deduplication:</strong> Applied clustering to eliminate redundant questions (reduced to 95,000)</li>
            <li><strong>Non-Technical Content Filtering:</strong> Removed grammar, formatting, and unprofessional comments (removed 41,000)</li>
            <li><strong>Specificity Filtering:</strong> Removed vague or speculative questions (removed 38,500)</li>
          </ul>
          <p>
            After filtering, the final dataset contained 15.5k questions drawn from 5,841 unique papers. The train dataset contains 13.2k questions and the test dataset contains 2.3k questions.
          </p>
        </div>
        <div class="columns is-vcentered">
          <div class="column">
            <img src="./static/images/intelliask/data_curation.png" alt="Data Curation Pipeline" style="width: 100%; border-radius: 10px;">
          </div>
          <div class="column">
            <img src="./static/images/intelliask/waterfall.png" alt="Filtering Pipeline" style="width: 100%; border-radius: 10px;">
          </div>
        </div>
      </div>
    </div>
    <!--/ Probe-15K Dataset. -->

    <!-- ProbeVote-500. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">ProbeVote-500: Human Preference Annotation</h2>
        <div class="content has-text-justified">
          <p>
            ProbeVote-500 consists of 572 annotated question–paper pairs abstracted from 300 randomly sampled ICLR 2025 submissions on OpenReview. For each paper, the full text was provided as input to large language models (Gemini 2.5 Flash, o3, Qwen2.5-32B), yielding one model-generated question per system. In parallel, the corresponding human-authored reviewer question from OpenReview was included as the reference.
          </p>
          <p>
            Human evaluators read each paper in full, including text, figures, and equations, to ensure proper context. Annotators then scored each anonymized question according to three binary dimensions:
          </p>
          <ul>
            <li><strong>Effort:</strong> Does the question demand real thought to answer? Low-effort questions can be answered by directly quoting the paper, whereas high-effort questions require synthesis and critical thinking.</li>
            <li><strong>Evidence:</strong> Is the question backed by specific content from the paper? High-evidence questions point to particular results, assumptions, or arguments.</li>
            <li><strong>Grounding:</strong> Is the question anchored in the actual content of the paper? Grounded questions refer to concrete methods, experiments, or claims across sections.</li>
          </ul>
        </div>
        <div class="columns is-vcentered">
          <div class="column">
            <img src="./static/images/intelliask/score_graph.png" alt="Score Distribution" style="width: 100%; border-radius: 10px;">
          </div>
        </div>
      </div>
    </div>
    <!--/ ProbeVote-500. -->

    <!-- IntelliReward. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">IntelliReward: Reward Model Architecture</h2>
        <div class="content has-text-justified">
          <p>
            Evaluating all 15,500 questions with human annotators across three rubrics is costly and risks bias from fatigue. To reduce reliance on manual effort, we trained IntelliReward on ProbeVote-500 to serve as an efficient and scalable substitute for human judgment.
          </p>
          <p>
            Our reward model handles multiple objectives by pairing a causal LLM with per-objective Transformer heads. We use <code>gpt-oss-20b</code> (medium reasoning) as the base. Given an input (e.g., paper OCR, generated question, task prompt), the LLM encodes it into a fixed representation. We extract the pooled hidden states of the last 50 output tokens and pass it to our per-objective Transformer head, which empirically improves performance compared to using an MLP head.
          </p>
          <p>
            Each evaluation objective (Effort, Evidence, Grounding) has an independent head producing logits. During training, the model minimizes the total loss across all objectives. IntelliReward achieves 72% mean accuracy, outperforming API-based LLM-as-judge baselines (Gemini 2.5 Flash: 37%, GPT-4.1: 32%, GPT-5: 53%).
          </p>
        </div>
        <div class="columns is-vcentered">
          <div class="column">
            <img src="./static/images/intelliask/reward_model.png" alt="IntelliReward Architecture" style="width: 100%; border-radius: 10px;">
          </div>
        </div>
      </div>
    </div>
    <!--/ IntelliReward. -->

    <!-- IntelliAsk. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">IntelliAsk: Training with Human-Aligned Rewards</h2>
        <div class="content has-text-justified">
          <p>
            As shown in our experiments, supervised fine-tuning (SFT) performs poorly for review question generation: the model copies surface style but does not produce questions with real effort, evidence, or grounding. To address this, we use our reward model, <strong>IntelliReward</strong>, to align generation with human preferences.
          </p>
          <p>
            We train with DAPO for IntelliAsk-7B and GRPO for IntelliAsk-32B: for each paper, the model generates several candidate questions, which are scored by IntelliReward, and these scores are used as rewards to guide optimization. The resulting model, <strong>IntelliAsk-32B</strong>, consistently outperforms SFT-only baselines by producing questions that are more evidence-based, better grounded, and require greater effort.
          </p>
        </div>
        <div class="columns is-vcentered">
          <div class="column">
            <img src="./static/images/intelliask/sft_vs_rl.png" alt="Training Curves" style="width: 100%; border-radius: 10px;">
          </div>
        </div>
      </div>
    </div>
    <!--/ IntelliAsk. -->


    <!-- Related Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Work</h2>

        <div class="content has-text-justified">
          <p>
            Recent research has increasingly explored the use of large language models (LLMs) to automate aspects of peer review. Several works train models on large corpora of reviews, often through supervised fine-tuning (SFT). For instance, <a href="https://aclanthology.org/2025.naacl-demo.44/">OpenReviewer</a> fine-tunes LLaMA-8B on 79K reviews to produce fluent and structured assessments, while <a href="https://aclanthology.org/2025.acl-long.1420/">DeepReview</a> develops a multi-stage pipeline that integrates retrieval and self-reflection.
          </p>
          <p>
            Other approaches explore multi-agent frameworks. <a href="https://arxiv.org/abs/2401.04259">MARG</a> distributes paper sections across specialized agents that collaborate to generate comprehensive feedback, while <a href="https://aclanthology.org/2024.findings-acl.580/">SWIF²T</a> decomposes review generation into planner, investigator, reviewer, and controller modules.
          </p>
          <p>
            Several datasets and evaluation frameworks also relate closely. <a href="https://aclanthology.org/2025.naacl-long.22/">PeerQA</a>, <a href="https://openreview.net/forum?id=DfhcOelEnP">cPAPERS</a>, and <a href="https://aclanthology.org/2024.emnlp-main.1163/">SciDQA</a> harvest reviewer questions and author responses, facilitating tasks such as answer generation or content retrieval rather than explicit question generation itself.
          </p>
          <p>
            Despite this progress, existing research overwhelmingly treats peer review as a problem of generating full reviews or answering reviewer questions. Very little attention has been given to <i>question generation itself</i>—the actionable and constructive element of peer feedback. Our work directly addresses this gap by introducing a human-annotated dataset of reviewer-style questions and training a specialized model for generating probing, useful questions in peer review.
          </p>
        </div>
      </div>
    </div>
    <!--/ Related Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{intelliask2026,
  author    = {First Author and Second Author},
  title     = {IntelliAsk: Learning to Ask Critical Questions with Human-Aligned Rewards},
  journal   = {ACL},
  year      = {2026},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This website is licensed under a <a rel="license"
                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        This website template is based on <a href="https://github.com/nerfies/nerfies.github.io">nerfies.github.io</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
