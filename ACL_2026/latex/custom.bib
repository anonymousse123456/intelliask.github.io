@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@inproceedings{one,
  author={Weixin Liang and Zachary Izzo and Yaohui Zhang and Haley Lepp and Hancheng Cao and Xuandong Zhao and Lingjiao Chen and Haotian Ye and Sheng Liu and Zhi Huang and Daniel A. McFarland and James Y. Zou},
  title={Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews},
  year={2024},
  cdate={1704067200000},
  url={https://openreview.net/forum?id=bX3J7ho18S},
  booktitle={ICML},
  crossref={conf/icml/2024}
}


@article{autorev,
  title   = {AutoRev: Automatic Peer Review System for Academic Research Papers},
  author  = {Maitreya Prafulla Chitale and Ketaki Mangesh Shetye and Harshit Gupta and Manav Chaudhary and Vasudeva Varma},
  year    = {2025},
  journal = {arXiv preprint arXiv: 2505.14376}
}

@article{qasper,
  title   = {A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
  author  = {Pradeep Dasigi and Kyle Lo and Iz Beltagy and Arman Cohan and Noah A. Smith and Matt Gardner},
  year    = {2021},
  journal = {arXiv preprint arXiv: 2105.03011}
}

@misc{cvpr_slides,
  author = {D. Davis},
  title = {CVPR 2021 training materials: Reference slides},
  year = {2021},
  url = {http://luthuli.cs.uiuc.edu/~daf/CVPR21TrainingMaterials/RefSlides.pdf}
}

@misc{iclr_review,
  author = {ICLR},
  title = {Leveraging LLM feedback to enhance review quality},
  year = {2025},
  month = {Apr},
  day = {15},
  url = {https://blog.iclr.cc/2025/04/15/leveraging-llm-feedback-to-enhance-review-quality/}
}

@misc{neurips_review,
  author = {NeurIPS},
  title = {Reviewer guidelines},
  year = {2023},
  url = {https://neurips.cc/Conferences/2023/ReviewerGuidelines}
}
@article{paper,
  title   = {Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025},
  author  = {Nitya Thakkar and Mert Yuksekgonul and Jake Silberg and Animesh Garg and Nanyun Peng and Fei Sha and Rose Yu and Carl Vondrick and James Zou},
  year    = {2025},
  journal = {arXiv preprint arXiv: 2504.09737}
}

@misc{olmocr,
      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},
      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},
      year={2025},
      eprint={2502.18443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18443},
}

@inproceedings{
li2024mediq,
title={MediQ: Question-Asking {LLM}s and a Benchmark for Reliable Interactive Clinical Reasoning},
author={Shuyue Stella Li and Vidhisha Balachandran and Shangbin Feng and Jonathan S. Ilgen and Emma Pierson and Pang Wei Koh and Yulia Tsvetkov},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=W4pIBQ7bAI}
}

@article{paperreview,
  publtype={informal},
  author={Hyungyu Shin and Jingyu Tang and Yoonjoo Lee and Nayoung Kim and Hyunseung Lim and Ji Yong Cho and Hwajung Hong and Moontae Lee and Juho Kim},
  title={Automatically Evaluating the Paper Reviewing Capability of Large Language Models},
  year={2025},
  month={February},
  cdate={1738368000000},
  journal={CoRR},
  volume={abs/2502.17086},
  url={https://doi.org/10.48550/arXiv.2502.17086}
}

@article{zhang2025llms,
  title   = {Can LLMs Ask Good Questions?},
  author  = {Yueheng Zhang and Xiaoyuan Liu and Yiyou Sun and Atheer Alharbi and Hend Alzahrani and Tianneng Shi and Basel Alomair and Dawn Song},
  year    = {2025},
  journal = {arXiv preprint arXiv: 2501.03491}
}

@inproceedings{wang-etal-2025-diversity,
    title = "Diversity-oriented Data Augmentation with Large Language Models",
    author = "Wang, Zaitian  and
      Zhang, Jinghan  and
      Zhang, Xinhao  and
      Liu, Kunpeng  and
      Wang, Pengfei  and
      Zhou, Yuanchun",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1084/",
    doi = "10.18653/v1/2025.acl-long.1084",
    pages = "22265--22283",
    ISBN = "979-8-89176-251-0"
}

@inproceedings{
ye2024language,
title={Language Models as Critical Thinking Tools: A Case Study of Philosophers},
author={Andre Ye and Jared Moore and Rose Novick and Amy X Zhang},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=ZZzXpyv65G}
}


@inproceedings{opr,
    title = "{O}pen{R}eviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews",
    author = "Idahl, Maximilian  and
      Ahmadi, Zahra",
    editor = "Dziri, Nouha  and
      Ren, Sean (Xiang)  and
      Diao, Shizhe",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-demo.44/",
    doi = "10.18653/v1/2025.naacl-demo.44",
    pages = "550--562",
    ISBN = "979-8-89176-191-9",
    abstract = "We present OpenReviewer, an open-source system for generating high-quality peer reviews of machine learning and AI conference papers. At its core is Llama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned on 79,000 expert reviews from top conferences. Given a PDF paper submission and review template as input, OpenReviewer extracts the full text, including technical content like equations and tables, and generates a structured review following conference-specific guidelines. Our evaluation on 400 test papers shows that OpenReviewer produces considerably more critical and realistic reviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other LLMs tend toward overly positive assessments, OpenReviewer{'}s recommendations closely match the distribution of human reviewer ratings. The system provides authors with rapid, constructive feedback to improve their manuscripts before submission, though it is not intended to replace human peer review. OpenReviewer is available as an online demo and open-source tool."
}


@inproceedings{zhu-etal-2025-deepreview,
    title = "{D}eep{R}eview: Improving {LLM}-based Paper Review with Human-like Deep Thinking Process",
    author = "Zhu, Minjun  and
      Weng, Yixuan  and
      Yang, Linyi  and
      Zhang, Yue",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1420/",
    doi = "10.18653/v1/2025.acl-long.1420",
    pages = "29330--29355",
    ISBN = "979-8-89176-251-0",
    abstract = "Large Language Models (LLMs) are increasingly utilized in scientific research assessment, particularly in automated paper review. However, existing LLM-based review systems face significant challenges, including limited domain expertise, hallucinated reasoning, and a lack of structured evaluation. To address these limitations, we introduce DeepReview, a multi-stage framework designed to emulate expert reviewers by incorporating structured analysis, literature retrieval, and evidence-based argumentation. Using DeepReview-13K, a curated dataset with structured annotations, we train DeepReviewer-14B, which outperforms CycleReviewer-70B with fewer tokens. In its best mode, DeepReviewer-14B achieves win rates of 88.21{\%} and 80.20{\%} against GPT-o1 and DeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper review, with all resources publicly available."
}


@misc{
tan2025peer,
title={Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions: Benchmarking Large Language Models},
author={Cheng Tan and Dongxin Lyu and Siyuan Li and Zhangyang Gao and Jingxuan Wei and Siqi Ma and Zicheng Liu and Stan Z. Li},
year={2025},
url={https://openreview.net/forum?id=uV3Gdoq2ez}
}

@misc{darcy2024margmultiagentreviewgeneration,
      title={MARG: Multi-Agent Review Generation for Scientific Papers}, 
      author={Mike D'Arcy and Tom Hope and Larry Birnbaum and Doug Downey},
      year={2024},
      eprint={2401.04259},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.04259}, 
}

@article{Zhuang_2025,
   title={Large language models for automated scholarly paper review: A survey},
   volume={124},
   ISSN={1566-2535},
   url={http://dx.doi.org/10.1016/j.inffus.2025.103332},
   DOI={10.1016/j.inffus.2025.103332},
   journal={Information Fusion},
   publisher={Elsevier BV},
   author={Zhuang, Zhenzhen and Chen, Jiandong and Xu, Hongfeng and Jiang, Yuwen and Lin, Jialiang},
   year={2025},
   month=dec, pages={103332} }

@inproceedings{baumgartner-etal-2025-peerqa,
    title = "{P}eer{QA}: A Scientific Question Answering Dataset from Peer Reviews",
    author = {Baumg{\"a}rtner, Tim  and
      Briscoe, Ted  and
      Gurevych, Iryna},
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.22/",
    doi = "10.18653/v1/2025.naacl-long.22",
    pages = "508--544",
    ISBN = "979-8-89176-189-6",
    abstract = "We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health.PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. We provide a detailed analysis of the collected dataset and conduct experiments establishing baseline systems for all three tasks. Our experiments and analyses reveal the need for decontextualization in document-level retrieval, where we find that even simple decontextualization approaches consistently improve retrieval performance across architectures. On answer generation, PeerQA serves as a challenging benchmark for long-context modeling, as the papers have an average size of 12k tokens."
}

@inproceedings{
sundar2024cpapers,
title={c{PAPERS}: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers},
author={Anirudh Sundar and Jin Xu and William Gay and Christopher Gordon Richardson and Larry Heck},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=DfhcOelEnP}
}

@inproceedings{singh-etal-2024-scidqa,
    title = "{S}ci{DQA}: A Deep Reading Comprehension Dataset over Scientific Papers",
    author = "Singh, Shruti  and
      Sarkar, Nandan  and
      Cohan, Arman",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1163/",
    doi = "10.18653/v1/2024.emnlp-main.1163",
    pages = "20908--20923",
    abstract = "Scientific literature is typically dense, requiring significant background knowledge and deep comprehension for effective engagement. We introduce SciDQA, a new dataset for reading comprehension that challenges language models to deeply understand scientific articles, consisting of 2,937 QA pairs. Unlike other scientific QA datasets, SciDQA sources questions from peer reviews by domain experts and answers by paper authors, ensuring a thorough examination of the literature. We enhance the dataset{'}s quality through a process that carefully decontextualizes the content, tracks the source document across different versions, and incorporates a bibliography for multi-document question-answering. Questions in SciDQA necessitate reasoning across figures, tables, equations, appendices, and supplementary materials, and require multi-document reasoning. We evaluate several open-source and proprietary LLMs across various configurations to explore their capabilities in generating relevant and factual responses, as opposed to simple review memorization. Our comprehensive evaluation, based on metrics for surface-level and semantic similarity, highlights notable performance discrepancies. SciDQA represents a rigorously curated, naturally derived scientific QA dataset, designed to facilitate research on complex reasoning within the domain of question answering for scientific texts."
}

@inproceedings{
ning2025pico,
title={Pi{CO}: Peer Review in {LLM}s based on Consistency Optimization},
author={Kun-Peng Ning and Shuo Yang and Yuyang Liu and Jia-Yu Yao and Zhenhui Liu and Yonghong Tian and Yibing Song and Li Yuan},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=sfQ6XpApfS}
}

@inproceedings{chamoun-etal-2024-automated,
    title = "Automated Focused Feedback Generation for Scientific Writing Assistance",
    author = "Chamoun, Eric  and
      Schlichtkrull, Michael  and
      Vlachos, Andreas",
    editor = "Ku, Lun-Wei  ands
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.580/",
    doi = "10.18653/v1/2024.findings-acl.580",
    pages = "9742--9763",
    abstract = "Scientific writing is a challenging task, particularly for novice researchers who often rely on feedback from experienced peers. Recent work has primarily focused on improving surface form and style rather than manuscript content. In this paper, we propose a novel task: automated focused feedback generation for scientific writing assistance. We present SWIF$^2$T: a Scientific WrIting Focused Feedback Tool. It is designed to generate specific, actionable and coherent comments, which identify weaknesses in a scientific paper and/or propose revisions to it. Our approach consists of four components - planner, investigator, reviewer and controller - leveraging multiple Large Language Models (LLMs) to implement them. We compile a dataset of 300 peer reviews citing weaknesses in scientific papers and conduct human evaluation. The results demonstrate the superiority in specificity, reading comprehension, and overall helpfulness of SWIF$^2$T{'}s feedback compared to other approaches. In our analysis, we also identified cases where automatically generated reviews were judged better than human ones, suggesting opportunities for integration of AI-generated feedback in scientific writing."
}

@inproceedings{du-etal-2024-llms,
    title = "{LLM}s Assist {NLP} Researchers: Critique Paper (Meta-)Reviewing",
    author = "Du, Jiangshu  and
      Wang, Yibo  and
      Zhao, Wenting  and
      Deng, Zhongfen  and
      Liu, Shuaiqi  and
      Lou, Renze  and
      Zou, Henry Peng  and
      Narayanan Venkit, Pranav  and
      Zhang, Nan  and
      Srinath, Mukund  and
      Zhang, Haoran Ranran  and
      Gupta, Vipul  and
      Li, Yinghui  and
      Li, Tao  and
      Wang, Fei  and
      Liu, Qin  and
      Liu, Tianlin  and
      Gao, Pengzhi  and
      Xia, Congying  and
      Xing, Chen  and
      Jiayang, Cheng  and
      Wang, Zhaowei  and
      Su, Ying  and
      Shah, Raj Sanjay  and
      Guo, Ruohao  and
      Gu, Jing  and
      Li, Haoran  and
      Wei, Kangda  and
      Wang, Zihao  and
      Cheng, Lu  and
      Ranathunga, Surangika  and
      Fang, Meng  and
      Fu, Jie  and
      Liu, Fei  and
      Huang, Ruihong  and
      Blanco, Eduardo  and
      Cao, Yixin  and
      Zhang, Rui  and
      Yu, Philip S.  and
      Yin, Wenpeng",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.292/",
    doi = "10.18653/v1/2024.emnlp-main.292",
    pages = "5081--5099",
    abstract = "Claim: This work is not advocating the use of LLMs for paper (meta-)reviewing. Instead, wepresent a comparative analysis to identify and distinguish LLM activities from human activities. Two research goals: i) Enable better recognition of instances when someone implicitly uses LLMs for reviewing activities; ii) Increase community awareness that LLMs, and AI in general, are currently inadequate for performing tasks that require a high level of expertise and nuanced judgment.This work is motivated by two key trends. On one hand, large language models (LLMs) have shown remarkable versatility in various generative tasks such as writing, drawing, and question answering, significantly reducing the time required for many routine tasks. On the other hand, researchers, whose work is not only time-consuming but also highly expertise-demanding, face increasing challenges as they have to spend more time reading, writing, and reviewing papers. This raises the question: how can LLMs potentially assist researchers in alleviating their heavy workload?This study focuses on the topic of LLMs as NLP Researchers, particularly examining the effectiveness of LLMs in assisting paper (meta-)reviewing and its recognizability. To address this, we constructed the ReviewCritique dataset, which includes two types of information: (i) NLP papers (initial submissions rather than camera-ready) with both human-written and LLM-generated reviews, and (ii) each review comes with ``deficiency'' labels and corresponding explanations for individual segments, annotated by experts. Using ReviewCritique, this study explores two threads of research questions: (i) ``LLMs as Reviewers'', how do reviews generated by LLMs compare with those written by humans in terms of quality and distinguishability? (ii) ``LLMs as Metareviewers'', how effectively can LLMs identify potential issues, such as Deficient or unprofessional review segments, within individual paper reviews? To our knowledge, this is the first work to provide such a comprehensive analysis."
}

@inproceedings{
chu2025sft,
title={{SFT} Memorizes, {RL} Generalizes: A Comparative Study of Foundation Model Post-training},
author={Tianzhe Chu and Yuexiang Zhai and Jihan Yang and Shengbang Tong and Saining Xie and Dale Schuurmans and Quoc V Le and Sergey Levine and Yi Ma},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=dYur3yabMj}
}

@misc{dapo,
      title={DAPO: An Open-Source LLM Reinforcement Learning System at Scale}, 
      author={Qiying Yu and Zheng Zhang and Ruofei Zhu and Yufeng Yuan and Xiaochen Zuo and Yu Yue and Weinan Dai and Tiantian Fan and Gaohong Liu and Lingjun Liu and Xin Liu and Haibin Lin and Zhiqi Lin and Bole Ma and Guangming Sheng and Yuxuan Tong and Chi Zhang and Mofan Zhang and Wang Zhang and Hang Zhu and Jinhua Zhu and Jiaze Chen and Jiangjie Chen and Chengyi Wang and Hongli Yu and Yuxuan Song and Xiangpeng Wei and Hao Zhou and Jingjing Liu and Wei-Ying Ma and Ya-Qin Zhang and Lin Yan and Mu Qiao and Yonghui Wu and Mingxuan Wang},
      year={2025},
      eprint={2503.14476},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.14476}, 
}

@misc{drop,
      title={DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs}, 
      author={Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},
      year={2019},
      eprint={1903.00161},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1903.00161}, 
}

@misc{musr,
      title={MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning}, 
      author={Zayne Sprague and Xi Ye and Kaj Bostrom and Swarat Chaudhuri and Greg Durrett},
      year={2024},
      eprint={2310.16049},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.16049}, 
}

@inproceedings{boolq,
  title =     {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author =    {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei, and Kwiatkowski, Tom and Collins, Michael, and Toutanova, Kristina},
  booktitle = {NAACL},
  year =      {2019},
}

@misc{gpqa,
      title={GPQA: A Graduate-Level Google-Proof Q&A Benchmark}, 
      author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      year={2023},
      eprint={2311.12022},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.12022}, 
}

@misc{writingbench,
      title={WritingBench: A Comprehensive Benchmark for Generative Writing}, 
      author={Yuning Wu and Jiahao Mei and Ming Yan and Chenliang Li and Shaopeng Lai and Yuran Ren and Zijia Wang and Ji Zhang and Mengyue Wu and Qin Jin and Fei Huang},
      year={2025},
      eprint={2503.05244},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2503.05244}, 
}

@misc{arenahard,
      title={From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline}, 
      author={Tianle Li and Wei-Lin Chiang and Evan Frick and Lisa Dunlap and Tianhao Wu and Banghua Zhu and Joseph E. Gonzalez and Ion Stoica},
      year={2024},
      eprint={2406.11939},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11939}, 
}

@misc{webgpt,
      title={WebGPT: Browser-assisted question-answering with human feedback}, 
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      eprint={2112.09332},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.09332}, 
}

@misc{trait,
      title={Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues}, 
      author={Lei Sun and Jinming Zhao and Qin Jin},
      year={2024},
      eprint={2409.19723},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.19723}, 
}

@inproceedings{xie2024calibrating,
  title     = {Calibrating Reasoning in Language Models with Internal Consistency},
  author    = {Zhihui Xie and Jizhou Guo and Tong Yu and Shuai Li},
  booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year      = {2024},
  url       = {https://openreview.net/forum?id=udZKVMPf3S}
}